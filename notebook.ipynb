{"cells":[{"cell_type":"markdown","metadata":{},"source":["# AIOps Demonstration Notebook\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 1: Setup Environment\n","!pip install numpy pandas matplotlib seaborn scikit-learn statsmodels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 2: Load and Explore Data\n","import pandas as pd\n","import numpy as np\n","import logging\n","\n","# Load your datasets\n","server_metrics = pd.read_csv(\"server_metrics.csv\")\n","business_metrics = pd.read_csv(\"business_metrics.csv\")\n","\n","# Display basic information and statistics\n","print(server_metrics.info())\n","print(server_metrics.describe())\n","\n","print(business_metrics.info())\n","print(business_metrics.describe())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 3: Data Preprocessing\n","\n","# Fill missing values with the mean for numeric columns, preserving the Timestamp column\n","server_metrics_numeric = server_metrics.select_dtypes(include=\"number\").fillna(\n","    server_metrics.select_dtypes(include=\"number\").mean()\n",")\n","server_metrics_non_numeric = server_metrics.select_dtypes(exclude=\"number\")\n","server_metrics = pd.concat([server_metrics_non_numeric, server_metrics_numeric], axis=1)\n","\n","business_metrics_numeric = business_metrics.select_dtypes(include=\"number\").fillna(\n","    business_metrics.select_dtypes(include=\"number\").mean()\n",")\n","business_metrics_non_numeric = business_metrics.select_dtypes(exclude=\"number\")\n","business_metrics = pd.concat(\n","    [business_metrics_non_numeric, business_metrics_numeric], axis=1\n",")\n","\n","# Print columns to ensure 'Timestamp' is preserved\n","print(\"Server Metrics Columns After Preprocessing:\")\n","print(server_metrics.columns)\n","\n","print(\"Business Metrics Columns After Preprocessing:\")\n","print(business_metrics.columns)\n","\n","print(\"Data Preprocessing Completed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 4: Data Analysis and Insights\n","# Clustering Analysis\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","\n","# Normalize/Scale Data\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","server_metrics_scaled = scaler.fit_transform(\n","    server_metrics.select_dtypes(include=\"number\")\n",")\n","\n","# Example: KMeans clustering\n","kmeans = KMeans(n_clusters=3, random_state=42)\n","server_clusters = kmeans.fit_predict(server_metrics_scaled)\n","\n","# Add cluster labels to the original dataframe\n","server_metrics[\"Cluster\"] = server_clusters\n","\n","# Calculate silhouette score for evaluating the clustering\n","silhouette_avg = silhouette_score(server_metrics_scaled, server_clusters)\n","print(f\"Silhouette Score: {silhouette_avg}\")\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Heatmap of correlations between server metrics\n","numeric_server_metrics = server_metrics.select_dtypes(include=\"number\")\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(numeric_server_metrics.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n","plt.title(\"Correlation between Server Metrics\")\n","plt.show()\n","\n","# Scatter plot of clusters for CPU and Memory Utilization\n","plt.figure(figsize=(10, 8))\n","plt.scatter(\n","    server_metrics[\"CPU Utilization (%)\"],\n","    server_metrics[\"Memory Utilization (%)\"],\n","    c=server_metrics[\"Cluster\"],\n","    cmap=\"viridis\",\n","    alpha=0.6,\n",")\n","plt.colorbar(label=\"Cluster\")\n","plt.xlabel(\"CPU Utilization (%)\")\n","plt.ylabel(\"Memory Utilization (%)\")\n","plt.title(\"Server Metrics Clustering\")\n","plt.show()\n","\n","# Pair plot for visualizing the clusters across multiple features\n","sns.pairplot(server_metrics, hue=\"Cluster\", diag_kind=\"kde\", markers=[\"o\", \"s\", \"D\"])\n","plt.suptitle(\"Pair Plot of Server Metrics by Cluster\", y=1.02)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","\n","\n","def get_pca_loadings(pca, features):\n","    \"\"\"\n","    Get PCA loadings for the specified features.\n","\n","    Args:\n","    - pca (PCA): Fitted PCA model.\n","    - features (list): List of features.\n","\n","    Returns:\n","    - pd.DataFrame: DataFrame containing PCA loadings.\n","    \"\"\"\n","    try:\n","        if pca.components_.shape[1] != len(features):\n","            raise ValueError(\"Mismatch between PCA components and feature length.\")\n","        loadings = pd.DataFrame(\n","            pca.components_.T, columns=[\"PC1\", \"PC2\"], index=features\n","        )\n","        return loadings\n","    except Exception as e:\n","        logging.error(f\"An error occurred while retrieving PCA loadings: {e}\")\n","        return pd.DataFrame()\n","\n","\n","# Ensure the correct columns are selected for scaling and PCA\n","numeric_server_metrics = server_metrics.select_dtypes(include=\"number\").drop(\n","    columns=[\"Cluster\"]\n",")\n","\n","# Normalize/Scale Data\n","scaler = StandardScaler()\n","server_metrics_scaled = scaler.fit_transform(numeric_server_metrics)\n","\n","\n","# PCA Analysis and Visualization\n","def apply_pca(scaled_data, clusters):\n","    try:\n","        pca = PCA(n_components=2)\n","        X_pca = pca.fit_transform(scaled_data)\n","        pca_df = pd.DataFrame(data=X_pca, columns=[\"PC1\", \"PC2\"])\n","        pca_df[\"Cluster\"] = clusters\n","\n","        plt.figure(figsize=(14, 10))\n","        sns.scatterplot(\n","            x=\"PC1\",\n","            y=\"PC2\",\n","            hue=\"Cluster\",\n","            palette=\"Set2\",\n","            data=pca_df,\n","            s=100,\n","            alpha=0.8,\n","            edgecolor=\"w\",\n","            linewidth=0.5,\n","        )\n","        plt.title(\"PCA of Server Metrics with Clusters\", fontsize=16, weight=\"bold\")\n","        plt.xlabel(\"Principal Component 1\", fontsize=14)\n","        plt.ylabel(\"Principal Component 2\", fontsize=14)\n","        plt.grid(True, linestyle=\"--\", linewidth=0.5)\n","        plt.legend(\n","            title=\"Cluster\",\n","            title_fontsize=\"13\",\n","            fontsize=\"11\",\n","            loc=\"upper right\",\n","            frameon=True,\n","            shadow=True,\n","            borderpad=1,\n","        )\n","        plt.show()\n","\n","        return pca_df, pca\n","    except Exception as e:\n","        logging.error(f\"An error occurred during PCA: {e}\")\n","        return None, None\n","\n","\n","# Apply PCA and visualize the clusters\n","pca_df, pca = apply_pca(server_metrics_scaled, server_clusters)\n","\n","# Check the shape of the scaled data and the original feature columns\n","print(f\"Shape of scaled data: {server_metrics_scaled.shape}\")\n","print(f\"Number of features: {len(numeric_server_metrics.columns)}\")\n","\n","# Get PCA loadings for feature interpretation\n","features = numeric_server_metrics.columns.tolist()\n","pca_loadings = get_pca_loadings(pca, features)\n","print(\"PCA Loadings:\")\n","print(pca_loadings)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 6: t-SNE Analysis and Visualization\n","\n","\n","def apply_tsne(scaled_data, clusters):\n","    \"\"\"\n","    Apply t-SNE to the scaled data and plot the results.\n","\n","    Args:\n","    - scaled_data (np.array): Scaled data.\n","    - clusters (np.array): Cluster labels.\n","\n","    Returns:\n","    - pd.DataFrame: DataFrame containing t-SNE results and cluster labels.\n","    \"\"\"\n","    try:\n","        tsne = TSNE(n_components=2, random_state=42)\n","        X_tsne = tsne.fit_transform(scaled_data)\n","        tsne_df = pd.DataFrame(data=X_tsne, columns=[\"TSNE1\", \"TSNE2\"])\n","        tsne_df[\"Cluster\"] = clusters\n","\n","        plt.figure(figsize=(14, 10))\n","        sns.scatterplot(\n","            x=\"TSNE1\",\n","            y=\"TSNE2\",\n","            hue=\"Cluster\",\n","            palette=\"Set2\",\n","            data=tsne_df,\n","            s=100,\n","            alpha=0.8,\n","            edgecolor=\"w\",\n","            linewidth=0.5,\n","        )\n","        plt.title(\"t-SNE of Server Metrics with Clusters\", fontsize=16, weight=\"bold\")\n","        plt.xlabel(\"t-SNE Component 1\", fontsize=14)\n","        plt.ylabel(\"t-SNE Component 2\", fontsize=14)\n","        plt.grid(True, linestyle=\"--\", linewidth=0.5)\n","        plt.legend(\n","            title=\"Cluster\",\n","            title_fontsize=\"13\",\n","            fontsize=\"11\",\n","            loc=\"upper right\",\n","            frameon=True,\n","            shadow=True,\n","            borderpad=1,\n","        )\n","        plt.show()\n","\n","        return tsne_df\n","    except Exception as e:\n","        logging.error(f\"An error occurred during t-SNE: {e}\")\n","        return None\n","\n","\n","# Apply t-SNE and visualize the clusters\n","tsne_df = apply_tsne(server_metrics_scaled, server_clusters)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Insights Based on PCA, t-SNE, and Clustering\n","\n","# Display silhouette score\n","print(f\"Silhouette Score: {silhouette_avg}\")\n","\n","# Display PCA loadings\n","print(\"PCA Loadings:\")\n","print(pca_loadings)\n","\n","\n","# Function to generate dynamic insights based on analysis\n","def generate_insights(\n","    server_metrics, silhouette_avg, pca_loadings, server_clusters, num_clusters=3\n","):\n","    insights = f\"\"\"\n","    ### Insights from PCA, t-SNE, and Clustering\n","\n","    1. **Silhouette Score**: The silhouette score of {silhouette_avg:.2f} indicates the average distance between clusters. A higher score suggests well-separated clusters, while a lower score indicates overlapping clusters.\n","\n","    2. **Cluster Distribution**:\n","    \"\"\"\n","    cluster_info = {}\n","    for cluster in range(num_clusters):\n","        cluster_data = server_metrics[server_metrics[\"Cluster\"] == cluster]\n","        cpu_utilization_mean = cluster_data[\"CPU Utilization (%)\"].mean()\n","        memory_utilization_mean = cluster_data[\"Memory Utilization (%)\"].mean()\n","        network_io_mean = cluster_data[\"Network I/O Throughput (Mbps)\"].mean()\n","        disk_io_mean = cluster_data[\"Disk I/O Throughput (MB/s)\"].mean()\n","        server_types = cluster_data[\"Server Configuration\"].value_counts().to_dict()\n","\n","        cluster_info[cluster] = {\n","            \"cpu_utilization_mean\": cpu_utilization_mean,\n","            \"memory_utilization_mean\": memory_utilization_mean,\n","            \"network_io_mean\": network_io_mean,\n","            \"disk_io_mean\": disk_io_mean,\n","            \"server_types\": server_types,\n","        }\n","\n","        insights += f\"\"\"\n","        - **Cluster {cluster}**:\n","            - Average CPU Utilization: {cpu_utilization_mean:.2f}%\n","            - Average Memory Utilization: {memory_utilization_mean:.2f}%\n","            - Average Network I/O Throughput: {network_io_mean:.2f} Mbps\n","            - Average Disk I/O Throughput: {disk_io_mean:.2f} MB/s\n","            - Server Configurations:\n","        \"\"\"\n","\n","        for config, count in server_types.items():\n","            insights += f\"            - {count} instances of {config}\\n\"\n","\n","    if not pca_loadings.empty:\n","        pc1_influences = pca_loadings[\"PC1\"].abs().sort_values(ascending=False)\n","        pc2_influences = pca_loadings[\"PC2\"].abs().sort_values(ascending=False)\n","        insights += f\"\"\"\n","        3. **PCA Analysis**:\n","            - **PC1 (Principal Component 1)** is primarily influenced by {pc1_influences.index[0]} ({pc1_influences.iloc[0]:.3f}) and {pc1_influences.index[1]} ({pc1_influences.iloc[1]:.3f}). This suggests that these metrics are key factors in explaining the variance in server performance.\n","            - **PC2 (Principal Component 2)** is strongly influenced by {pc2_influences.index[0]} ({pc2_influences.iloc[0]:.3f}) and {pc2_influences.index[1]} ({pc2_influences.iloc[1]:.3f}). This indicates that these metrics are critical in differentiating the performance characteristics of the servers.\n","        \"\"\"\n","    else:\n","        insights += \"\"\"\n","        3. **PCA Analysis**:\n","            - PCA loadings could not be calculated, so insights from PCA are not available.\n","        \"\"\"\n","\n","    insights += f\"\"\"\n","    4. **t-SNE Visualization**:\n","        - The t-SNE plot shows clear separation between the clusters, confirming that the clustering is meaningful and well-defined.\n","\n","    ### Recommendations\n","\n","    1. **Optimize Memory Utilization**: For servers in clusters with high memory utilization, focus on optimizing memory usage to prevent potential bottlenecks and improve overall performance.\n","\n","    2. **Load Balancing for High CPU Utilization Servers**: For servers in clusters with high CPU utilization, consider load balancing strategies to distribute the load more evenly and prevent CPU overutilization.\n","\n","    3. **Monitor Disk I/O**: Given the significant influence of Disk I/O Throughput on PC2, it's important to monitor and optimize disk performance, especially for servers with high disk activity.\n","\n","    4. **Regular Performance Reviews**: Periodically review server performance metrics to ensure that the clusters remain balanced and to identify any emerging performance issues.\n","\n","    By implementing these recommendations, you can enhance server performance, improve resource utilization, and ensure a more balanced load across your server infrastructure.\n","    \"\"\"\n","\n","    return insights, cluster_info\n","\n","\n","# Generate insights based on the current analysis\n","dynamic_insights, cluster_info = generate_insights(\n","    server_metrics, silhouette_avg, pca_loadings, server_clusters\n",")\n","print(dynamic_insights)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 7: Business Analysis\n","\n","# Ensure 'Timestamp' column is present and convert to datetime\n","if \"Timestamp\" not in server_metrics.columns:\n","    raise KeyError(\"The 'Timestamp' column is missing in the server_metrics DataFrame.\")\n","if \"Timestamp\" not in business_metrics.columns:\n","    raise KeyError(\n","        \"The 'Timestamp' column is missing in the business_metrics DataFrame.\"\n","    )\n","\n","server_metrics[\"Timestamp\"] = pd.to_datetime(server_metrics[\"Timestamp\"])\n","business_metrics[\"Timestamp\"] = pd.to_datetime(business_metrics[\"Timestamp\"])\n","\n","# Aligning data based on Timestamp\n","server_metrics.set_index(\"Timestamp\", inplace=True)\n","business_metrics.set_index(\"Timestamp\", inplace=True)\n","\n","# Ensure both dataframes are aligned\n","aligned_data = server_metrics.join(\n","    business_metrics, how=\"inner\", lsuffix=\"_server\", rsuffix=\"_business\"\n",")\n","\n","# Print columns to check names after joining\n","print(\"Aligned Data Columns:\")\n","print(aligned_data.columns)\n","\n","# Check for and drop missing values, if any\n","aligned_data.dropna(inplace=True)\n","\n","# Split back into server and business metrics\n","server_cols = [\n","    \"CPU Utilization (%)\",\n","    \"Memory Utilization (%)\",\n","    \"Network I/O Throughput (Mbps)\",\n","    \"Disk I/O Throughput (MB/s)\",\n","]\n","business_cols = [\n","    \"Response Time (ms)\",\n","    \"Customer Satisfaction (CSAT)\",\n","    \"Operational Costs ($)\",\n","    \"Service Uptime (%)\",\n","]\n","\n","numeric_server_metrics = aligned_data[server_cols]\n","numeric_business_metrics = aligned_data[business_cols]\n","\n","# Rename columns for easier access\n","numeric_server_metrics.columns = [\n","    \"CPU Utilization (%)\",\n","    \"Memory Utilization (%)\",\n","    \"Network I/O Throughput (Mbps)\",\n","    \"Disk I/O Throughput (MB/s)\",\n","]\n","numeric_business_metrics.columns = [\n","    \"Response Time (ms)\",\n","    \"Customer Satisfaction (CSAT)\",\n","    \"Operational Costs ($)\",\n","    \"Service Uptime (%)\",\n","]\n","\n","# Compute the correlation matrix between server metrics and business metrics\n","correlation_matrix = numeric_server_metrics.join(numeric_business_metrics).corr()\n","print(\"Correlation with Business Metrics:\")\n","print(correlation_matrix)\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Heatmap of correlations between server metrics and business metrics\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n","plt.title(\"Correlation between Server Metrics and Business Metrics\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Step 8: Optimization Recommendations & Business Insights\n","\n","# Scatter plot function\n","def plot_scatter(x, y, clusters, xlabel, ylabel, title):\n","    plt.figure(figsize=(10, 8))\n","    plt.scatter(x, y, c=clusters, cmap=\"viridis\", alpha=0.6)\n","    plt.colorbar(label=\"Cluster\")\n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.title(title)\n","    plt.show()\n","\n","\n","# Assuming aligned_data and correlation_matrix are defined appropriately\n","\n","# Define scatter plots with feature pairs and titles\n","scatter_plots = [\n","    (\n","        \"CPU Utilization (%)\",\n","        \"Customer Satisfaction (CSAT)\",\n","        \"CPU Utilization vs Customer Satisfaction\",\n","    ),\n","    (\n","        \"Memory Utilization (%)\",\n","        \"Response Time (ms)\",\n","        \"Memory Utilization vs Response Time\",\n","    ),\n","    (\n","        \"Network I/O Throughput (Mbps)\",\n","        \"Operational Costs ($)\",\n","        \"Network I/O Throughput vs Operational Costs\",\n","    ),\n","    (\n","        \"Disk I/O Throughput (MB/s)\",\n","        \"Service Uptime (%)\",\n","        \"Disk I/O Throughput vs Service Uptime\",\n","    ),\n","]\n","\n","# Generate Scatter Plots\n","for x_feature, y_feature, plot_title in scatter_plots:\n","    plot_scatter(\n","        aligned_data[x_feature],\n","        aligned_data[y_feature],\n","        aligned_data[\"Cluster\"],\n","        x_feature,\n","        y_feature,\n","        plot_title,\n","    )\n","\n","# Generate Insights dynamically\n","insights = {}\n","for x_feature, y_feature, _ in scatter_plots:\n","    correlation = correlation_matrix.loc[x_feature, y_feature]\n","    insights[f\"{x_feature} and {y_feature}\"] = correlation\n","\n","print(\"\\nInsights:\")\n","for k, v in insights.items():\n","    print(f\"{k}: {v:.2f}\")\n","\n","\n","#  Automated Insights Interpretation\n","def interpret_insights(insights):\n","    for key, correlation in insights.items():\n","        feature1, feature2 = key.split(\" and \")\n","        if correlation > 0.5:\n","            print(\n","                f\"A strong positive correlation of {correlation:.2f} between {feature1} and {feature2} suggests that as {feature1} increases, {feature2} also increases.\"\n","            )\n","        elif correlation < -0.5:\n","            print(\n","                f\"A strong negative correlation of {correlation:.2f} between {feature1} and {feature2} suggests that as {feature1} increases, {feature2} decreases.\"\n","            )\n","        else:\n","            print(\n","                f\"A weak correlation of {correlation:.2f} between {feature1} and {feature2} suggests that changes in {feature1} have little impact on {feature2}.\"\n","            )\n","\n","\n","print(\"\\nInterpretation of Insights:\")\n","interpret_insights(insights)\n","\n","# Function to generate optimization recommendations based on correlations and cluster info\n","def optimization_recommendations(correlations, cluster_info):\n","    recommendations = []\n","    for key, correlation in correlations.items():\n","        feature1, feature2 = key.split(\" and \")\n","        if correlation > 0.5:\n","            action = \"optimize\"\n","        elif correlation < -0.5:\n","            action = \"minimize\"\n","        else:\n","            continue\n","\n","        for cluster, info in cluster_info.items():\n","            recommendations.append(\n","                f\"For Cluster {cluster}, {action} {feature1} to improve {feature2}. \"\n","                f\"Cluster {cluster} contains server configurations: {', '.join([f'{count} instances of {config}' for config, count in info['server_types'].items()])}.\"\n","            )\n","\n","    return recommendations\n","\n","\n","print(\"\\nOptimization Recommendations:\")\n","for recommendation in optimization_recommendations(insights, cluster_info):\n","    print(f\"- {recommendation}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
